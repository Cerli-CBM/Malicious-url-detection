# -*- coding: utf-8 -*-
"""Malicious_urlbda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C1-WXgc5gPu2A5hiENN_EzdSTXM0yNgv

**MALICIOUS URL DETECTION**

The goal of this project is to develop a malicious URL detection system that identifies
potentially harmful URLs to users.
"""

!pip install pyspark

pip install findspark

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better
spark

#importing datasets
md =spark.read.csv("/content/drive/MyDrive/malicious_url.csv",header=True,inferSchema=True)

md.show(10,False)

"""**ACTIONS**"""

#count the number of rows
md.count()

#take -retrieve the first n elements
md.take(3)

#first- retrieve the first row in the dataset
md.first()

#limit - limit the number of rows
md.limit(10)

#retrieves the columns in the dataset
md.columns

#returns the datatypes of every features in the dataset
md.dtypes

md.printSchema()

#calculate the number of unique values in the "source"
from pyspark.sql import functions as func
source_unique_count = md.select(func.size(func.collect_set("source"))).collect()[0][0]
print(f'Number of unique sources: {source_unique_count}')

#corr- calculates the correlation coefficient between url_len and url_entropy
correlation =md.stat.corr("url_len", "url_entropy")
print(f'Correlation between url_len and url_entropy: {correlation}')

#mean - calculates the mean value of the url-len
mean_url_len = md.select(func.mean("url_len")).collect()[0][0]
print(f'Mean url length: {mean_url_len}')

#min - calculates the minimum value of the url_hamming
min_url_hamming_1 =md.select(func.min("url_hamming_1")).collect()[0][0]
print(f'Minimum url hamming_1: {min_url_hamming_1}')

#max - calculates the maximum value of the url_entropy
max_url_entropy = md.select(func.max("url_entropy")).collect()[0][0]
print(f'Maximum url entropy: {max_url_entropy}')

#describe - returns the statistical summary of url_len and url_entropy
stats = md.describe(['url_len', 'url_entropy'])
stats

"""**TRANSFORMATIONS**"""

#calculates the average 'url_len' for each distinct value in the 'source'
grouped_data = md.groupBy('source').agg({'url_len': 'mean'})
grouped_data

#creates a new DataFrame 'md1' by adding a constant value of 10 to the values in the 'url_len' column of the original DataFrame 'md'.
from pyspark.sql.functions import col, lit
md1 = md.withColumn('new_column', col('url_len') + lit(10))
md1

#Rename the source colum into datasource
renamed_data = md.withColumnRenamed('source', 'data_source')
renamed_data

#sort the url_len value in ascending order
sorted_data = md.orderBy('url_len', ascending=False)
sorted_data

#creates a df 'filtered' by selecting rows from the original DataFrame 'md' where the URL length is greater than 50 characters and the count of dots in the URL is three or more.

filterd=md.filter((col("url_len") > 50) & (col("url_count_dot") >= 3))
filterd

#extracts key-value pairs for all columns except 'source' and 'url', and returns these pairs as a list of tuples
def extract_values(row):
    values = []
    for key, value in row.asDict().items():
        if key not in ['source', 'url']:
            values.append((f"{key}_value", value))
    return values

# Apply flatMap transformation
flat_mapped_rdd = md.rdd.flatMap(extract_values)

# Convert the RDD back to DataFrame for better visualization
flat_mapped_df = flat_mapped_rdd.toDF(['feature', 'value'])
flat_mapped_df.show()

#isNull(),filter(): to check null values in all the columns
from pyspark.sql.functions import col
for col_name in md.columns:
    null_count = md.filter(col(col_name).isNull()).count()
    print(f"Column '{col_name}' has {null_count} null values.")

"""**SPARK SQL**"""

#drops the duplicates in the dataset
dropdup = md.dropDuplicates()
dropdup.show(10)

#Show all entries in source column
select_md=md.select("source").show(10)

#creates a new df 'when_sql' by adding a column that assigns a value of 1 when the 'source' column matches 'ALL-phishing-domains' and 0 otherwise
from pyspark.sql.functions import *
from pyspark.sql.types import *
when_sql=md.select("source", when(md.source == 'ALL-phishing-domains', 1).otherwise(0)).show()

#isin - filters the dataset to include rows where the 'source' column values are either "dmoz_harvard" or "Edomcop"
isin_sql=md [md.source.isin("dmoz_harvard", "Edomcop")].show()

#like - checks if the 'source' column contains the substring 'd'
like_sql=md.select("url_len", "source", md.source.like("% d %")).show()

#startswith - check if the 'source' column starts with the string "ALL".
start=md.select("url_len", "source", md.source.startswith("ALL")).show(5)

#endswith- check if the 'source' column ends with the string "rd".
ends=md.select("url_len", "source", md.source.endswith("rd")).show()

#substr- Extracts a substring starting from the first character and taking the next 10 characters
substr=md.select(md.url.substr(1, 10).alias("url")).show()

#calculates the count of each distinct value in the 'source' column
group=md.groupBy("source").count().show()

#show only rows where the 'source' column is equal to 'domcop'
filter_sql=md.filter(md["source"] == 'domcop').show(5)

# Write & Save File in .json format
md.select("source", "url","url_len","url_has_login","url_has_server") \
        .write \
        .save("Url.json",format="json")

md_output = spark.read.format("json").load("Url.json")

# Show the output DataFrame
md_output.show()

"""**Importing libraries**"""

from pyspark.ml.feature import VectorAssembler,StringIndexer
from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier

#Rearrange the dataset
data = md.select('url','source', 'url_has_login', 'url_has_client', 'url_has_server', 'url_has_admin', 'url_has_ip', 'url_isshorted', 'url_len', 'url_entropy', 'url_hamming_1', 'url_hamming_00', 'url_hamming_10', 'url_hamming_01', 'url_hamming_11', 'url_2bentropy', 'url_3bentropy', 'url_count_dot', 'url_count_https', 'url_count_http', 'url_count_perc', 'url_count_hyphen', 'url_count_www', 'url_count_atrate', 'url_count_hash', 'url_count_semicolon', 'url_count_underscore', 'url_count_ques', 'url_count_equal', 'url_count_amp', 'url_count_letter', 'url_count_digit', 'url_count_sensitive_financial_words', 'url_count_sensitive_words', 'url_nunique_chars_ratio', 'path_len', 'path_count_no_of_dir', 'path_count_no_of_embed', 'path_count_zero', 'path_count_pertwent', 'path_has_any_sensitive_words', 'path_count_lower', 'path_count_upper', 'path_count_nonascii', 'path_has_singlechardir', 'path_has_upperdir', 'query_len', 'query_count_components', 'pdomain_len', 'pdomain_count_hyphen', 'pdomain_count_atrate', 'pdomain_count_non_alphanum', 'pdomain_count_digit', 'tld_len', 'tld', 'tld_is_sus', 'pdomain_min_distance', 'subdomain_len', 'subdomain_count_dot','label')

data.dtypes

data.groupBy('label').count().show()

import matplotlib.pyplot as plt
#boxplot to compare the distribution of URL lengths between different categories (e.g., malicious vs. non-malicious).
plt.figure(figsize=(10, 6))
md.select('source', 'url_len').toPandas().boxplot(column='url_len', by='source')
plt.xlabel('Category')
plt.ylabel('URL Length')
plt.title('Boxplot of URL Length by Category')
plt.show()

#pie chart to visualize the proportion of URLs from different sources in the dataset.
sources_count = md.groupBy('source').count().orderBy('count', ascending=False).toPandas()

plt.figure(figsize=(8, 8))
plt.pie(sources_count['count'], labels=sources_count['source'], autopct='%1.1f%%')
plt.title('Proportion of URLs by Source')
plt.show()

#unique values for source
data.select("source").distinct().show()

#Converting the string into numerical
sourceEncode = StringIndexer(inputCol = "source",outputCol ="source_index").fit(data)
data = sourceEncode.transform(data)

#after encoding
data.select("source_index").distinct().show()

"""**FEATURE SELECTION**"""

df = data.select('source','source_index','url_has_login','url_has_client','url_has_server','url_len','label')

df.show()

#VectorAssembler
req_features =  ['source_index','url_has_login','url_has_client','url_has_server','url_len']

vec_assemb = VectorAssembler(inputCols=req_features,outputCol ="features")

vec_df = vec_assemb.transform(df)
vec_df.show()

"""**MODEL BUILDING**"""

#Train test split
train_df,test_df = vec_df.randomSplit([0.7,0.3])

train_df.count()

test_df.count()

train_df.describe()

"""**PCA**"""

from pyspark.ml.feature import PCA


numericColsAll  = ['url_has_login','url_has_client','url_has_server','url_len']
label = 'label'
assembler = VectorAssembler(inputCols=numericColsAll , outputCol="Numfeatures")
df = assembler.transform(md)

    # Apply PCA
pca = PCA(k=2, inputCol="Numfeatures", outputCol="pca_features")
model = pca.fit(df)
result = model.transform(df)
result.select("Numfeatures", "pca_features").show(truncate=False)

"""**LOGISTIC REGRESSION**"""

lr = LogisticRegression(featuresCol ="features",labelCol="label")

lr_model = lr.fit(train_df)

y_pred1 = lr_model.transform(test_df)

y_pred1.show()

y_pred1.select('label','rawPrediction', 'probability', 'prediction').show()

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
mult_eval =MulticlassClassificationEvaluator(labelCol="label",metricName="accuracy")


accuracy = mult_eval.evaluate(y_pred1)
print("Accuracy : " ,accuracy)

"""**DECISIONTREE CLASSIFIER**"""

#Decision tree
dc = DecisionTreeClassifier(featuresCol ="features",labelCol="label")
dc_model = dc.fit(train_df)
y_pred = dc_model.transform(test_df)

y_pred.show()

y_pred.select('label','rawPrediction', 'probability', 'prediction').show()

mult_eval =MulticlassClassificationEvaluator(labelCol="label",metricName="accuracy")

dt_accuracy = mult_eval.evaluate(y_pred)

print("Decision Tree Accuracy : ",dt_accuracy)

from tabulate import tabulate

# Sample accuracy scores for 4 models
model_names = ['Logistic regression', 'Decision Tree']
accuracy_scores = [0.83, 0.99]

# Create a list of tuples for the data and sort in descending order based on accuracy scores
data = sorted(list(zip(model_names, accuracy_scores)), key=lambda x: x[1], reverse=True)

# Create the table using tabulate
table = tabulate(data, headers=['Model', 'Accuracy Score'], tablefmt='fancy_grid')

# Print the table
print(table)

"""INFERENCE : The DecisionTree model (99% accuracy) outperforms the logistic model (83% accuracy)in terms of accuracy, indicating that it may be a better choice for predicting the malicious url that identifies the potentially harmful URLs to users."""